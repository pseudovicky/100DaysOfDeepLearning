{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184cae5f",
   "metadata": {},
   "source": [
    "\n",
    "LLMs Don’t Have Memory — So How Do They Remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8aeb33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3324769014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()  # Load environment variables from .env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577eb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a26259",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "LLMs are stateless by nature, but we can add memory through:\n",
    "1. **Passing conversation history** in each API call\n",
    "2. **Memory management systems** that handle history automatically\n",
    "3. **Different strategies** based on use case (buffer, window, summary)\n",
    "\n",
    "The key is choosing the right memory type based on your needs for cost, context length, and detail retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba725f56",
   "metadata": {},
   "source": [
    "## 7. Comparing Memory Types\n",
    "\n",
    "| Memory Type | Pros | Cons | Use Case |\n",
    "|-------------|------|------|----------|\n",
    "| **Buffer** | Simple, complete history | Expensive for long conversations | Short conversations |\n",
    "| **Buffer Window** | Controls cost, recent context | Loses older information | Fixed context needs |\n",
    "| **Summary** | Efficient for long conversations | May lose details | Long conversations |\n",
    "| **Summary Buffer** | Best of both worlds | More complex | Production applications |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View message history for session\n",
    "print(\"\\nMessage History:\")\n",
    "for message in store[\"user_123\"].messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chain with session\n",
    "config = {\"configurable\": {\"session_id\": \"user_123\"}}\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi, I'm Emma and I love Python programming\"},\n",
    "    config=config\n",
    ")\n",
    "print(\"Response 1:\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name and what do I love?\"},\n",
    "    config=config\n",
    ")\n",
    "print(\"\\nResponse 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911305d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "# Store for session histories\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create prompt with message placeholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Wrap with message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e36074",
   "metadata": {},
   "source": [
    "## 6. Using Message History with LCEL (LangChain Expression Language)\n",
    "Modern approach using RunnableWithMessageHistory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70820581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# Summary buffer with token limit\n",
    "summary_buffer_memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "conversation_sb = ConversationChain(llm=llm, memory=summary_buffer_memory, verbose=True)\n",
    "\n",
    "print(conversation_sb.predict(input=\"Hello! I'm learning about AI\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_sb.predict(input=\"Specifically, I'm interested in LLM memory mechanisms\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_sb.predict(input=\"Can you summarize what we've discussed?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14477df",
   "metadata": {},
   "source": [
    "## 5. Conversation Summary Buffer Memory\n",
    "Combines buffer and summary - keeps recent messages and summarizes older ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04767f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the summary\n",
    "print(\"Summary:\")\n",
    "print(summary_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Create summary memory\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "conversation_summary = ConversationChain(llm=llm, memory=summary_memory, verbose=True)\n",
    "\n",
    "print(conversation_summary.predict(input=\"Hi, I'm Charlie. I work as a data scientist\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_summary.predict(input=\"I'm currently working on a project about LLM memory\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_summary.predict(input=\"What do you know about me?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc4895c",
   "metadata": {},
   "source": [
    "## 4. Conversation Summary Memory\n",
    "Progressively summarizes the conversation to save tokens while retaining context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d54a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Keep only last 2 interactions (k=2)\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "conversation_window = ConversationChain(llm=llm, memory=window_memory, verbose=True)\n",
    "\n",
    "print(conversation_window.predict(input=\"Hi, my name is Bob\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_window.predict(input=\"I love pizza\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_window.predict(input=\"I also like ice cream\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_window.predict(input=\"What's my name?\"))  # Should remember\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation_window.predict(input=\"What food did I mention first?\"))  # Might forget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ecc038",
   "metadata": {},
   "source": [
    "## 3. Conversation Buffer Window Memory\n",
    "Only keeps the last K interactions to limit context size and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the conversation history\n",
    "print(\"Chat History:\")\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Create memory and chain\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "# Now the model remembers context\n",
    "print(conversation.predict(input=\"Hi, my name is Alice\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation.predict(input=\"What's my name?\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(conversation.predict(input=\"What did I just tell you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e709f",
   "metadata": {},
   "source": [
    "## 2. Conversation Buffer Memory\n",
    "Stores the entire conversation history. Simple but can get expensive with long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c04b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without memory - each call is independent\n",
    "response1 = llm.invoke(\"Hi, my name is Alice\")\n",
    "print(\"Response 1:\", response1.content)\n",
    "\n",
    "response2 = llm.invoke(\"What's my name?\")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a474c4b",
   "metadata": {},
   "source": [
    "## 1. Basic Chat Without Memory\n",
    "Without memory, the LLM doesn't remember previous messages in the conversation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
